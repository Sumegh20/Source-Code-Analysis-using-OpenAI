{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.memory import ConversationSummaryMemory\n",
    "# from langchain.chains import ConversationalRetrievalChain\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import AIMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.environ.get('OPENAI_API_KEY')\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.environ.get('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clone an repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<git.repo.base.Repo '/home/sumgh/Data Science/projects/code-analyzer/research/test_repo/.git'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_path = \"test_repo/\"\n",
    "\n",
    "Repo.clone_from(\"https://github.com/Sumegh20/myIPYNBrenderer.git\", to_path=repo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GenericLoader.from_filesystem(path=\"test_repo\",\n",
    "                                        glob = \"**/*\",\n",
    "                                       suffixes=[\".py\"],\n",
    "                                       parser = LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
    ")\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: RecursiveCharacterTextSplitter does not support multiple languages in one shot. For this, we need to write a custom RecursiveCharacterTextSplitter class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_splitter = RecursiveCharacterTextSplitter.from_language(language = Language.PYTHON,\n",
    "                                                             chunk_size = 1000,\n",
    "                                                             chunk_overlap = 200)\n",
    "\n",
    "text_splitter = documents_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embdding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sumgh/Data Science/projects/code-analyzer/code_env/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "embeddings=OpenAIEmbeddings(disallowed_special=())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = \"faiss_index\"\n",
    "vectordb = FAISS.from_documents(text_splitter, embeddings)\n",
    "vectordb.save_local(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kn_base = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=kn_base.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"\"\"You are an assistant for question-answering tasks while prioritizing a seamless user experience.\n",
    "            Use the following pieces of retrieved context to answer the question.\n",
    "            If you don't know the answer, just say that you don't know.\n",
    "            Use five sentences maximum and keep the answer concise.\n",
    "            You should be able to remember and reference the last three conversations between you and the user.\n",
    "            Maintain a friendly, positive, and professional tone throughout interactions.\n",
    "            Question: {question}\n",
    "            Context: {context}\n",
    "            Chat history: {chat_history}\n",
    "            Answer:\n",
    "        \"\"\"\n",
    "prompt=ChatPromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sumgh/Data Science/projects/code-analyzer/code_env/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0) #, streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_handeler(input: dict):\n",
    "    return input['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "            # {\"context\": retriever,  \"question\": RunnablePassthrough()}\n",
    "            RunnablePassthrough().assign(\n",
    "                context = input_handeler | retriever\n",
    "            )\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your second previous question was about explaining the render_site function.\n"
     ]
    }
   ],
   "source": [
    "question = \"What was my second previous question?\"\n",
    "responce = rag_chain.invoke({\"question\": question, \"chat_history\":chat_history})\n",
    "\n",
    "print(responce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            role=\"user\", \n",
    "            content=question\n",
    "        ),\n",
    "        AIMessage(\n",
    "            role=\"assistant\",\n",
    "            content=responce\n",
    "        ),\n",
    "    ]  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constant import file_extensions, file_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Language.PYTHON: 'python'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language = 'PYTHON'\n",
    "file_language.get(language, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clone any github repositories \n",
    "def repo_ingestion(repo_url):\n",
    "    os.makedirs(\"repo\", exist_ok=True)\n",
    "    repo_path = \"repo/\"\n",
    "    Repo.clone_from(repo_url, to_path=repo_path)\n",
    "\n",
    "\n",
    "# Create suffixes from language\n",
    "def get_suffixes_from_language(language):\n",
    "    suffixes = file_extensions.get(language, None)\n",
    "    return suffixes\n",
    "\n",
    "\n",
    "def get_language_from_language(language):\n",
    "    Languages = file_language.get(language, None)\n",
    "    return Languages\n",
    "\n",
    "\n",
    "#Loading repositories as documents\n",
    "def create_document_from_repo(repo_path, language):\n",
    "    suffixes = get_suffixes_from_language(language)\n",
    "    Languages = get_language_from_language(language)\n",
    "\n",
    "    loader = GenericLoader.from_filesystem(repo_path,\n",
    "                                        glob = \"**/*\",\n",
    "                                       suffixes=suffixes,\n",
    "                                       parser = LanguageParser(language=Languages, parser_threshold=500)\n",
    "                                        )\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "\n",
    "#Creating text chunks \n",
    "def text_splitter(documents):\n",
    "    documents_splitter = RecursiveCharacterTextSplitter.from_language(language = Language.PYTHON,\n",
    "                                                             chunk_size = 1000,\n",
    "                                                             chunk_overlap = 200)\n",
    "    \n",
    "    text_chunks = documents_splitter.split_documents(documents)\n",
    "    return text_chunks\n",
    "\n",
    "\n",
    "#loading embeddings model\n",
    "def load_embedding():\n",
    "    embeddings=OpenAIEmbeddings(disallowed_special=())\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "#creating knowlagebase\n",
    "def create_knowledgebase(texts, db_path=\"faiss_index\"):\n",
    "    embeddings = load_embedding()\n",
    "\n",
    "    vectordb = FAISS.from_documents(texts, embeddings)\n",
    "    vectordb.save_local(db_path)\n",
    "\n",
    "\n",
    "#loading knowledgebase\n",
    "def get_knowledge_base(db_path, embedding):\n",
    "    vectordb = FAISS.load_local(db_path, embedding, allow_dangerous_deserialization=True)\n",
    "\n",
    "    return vectordb\n",
    "\n",
    "\n",
    "#Create Prompt\n",
    "def create_prompt():\n",
    "    template=\"\"\"You are an assistant for question-answering tasks while prioritizing a seamless user experience.\n",
    "            Use the following pieces of retrieved context to answer the question.\n",
    "            If you don't know the answer, just say that you don't know.\n",
    "            Use five sentences maximum and keep the answer concise.\n",
    "            You should be able to remember and reference the last three conversations between you and the user.\n",
    "            Maintain a friendly, positive, and professional tone throughout interactions.\n",
    "            Question: {question}\n",
    "            Context: {context}\n",
    "            Chat history: {chat_history}\n",
    "            Answer:\n",
    "        \"\"\"\n",
    "    prompt=ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# input handeler\n",
    "def input_handeler(input: dict):\n",
    "    return input['question']\n",
    "\n",
    "\n",
    "# create RAG chain\n",
    "def create_rag_chain(llm, retriever, prompt):\n",
    "    rag_chain = (\n",
    "            # {\"context\": retriever,  \"question\": RunnablePassthrough()}\n",
    "            RunnablePassthrough().assign(\n",
    "                context = input_handeler | retriever\n",
    "            )\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "    \n",
    "    return rag_chain\n",
    "\n",
    "\n",
    "def get_llm():\n",
    "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, streaming=True)\n",
    "\n",
    "    return llm\n",
    "\n",
    "#user_input\n",
    "def get_output_stream(question, chat_history):\n",
    "    embedding = load_embedding()\n",
    "    vectordb = get_knowledge_base(db_path=database_path, embedding=embedding)\n",
    "    llm = get_llm()\n",
    "    retriever = vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":3})\n",
    "    prompt = create_prompt()\n",
    "\n",
    "    rag_chain = create_rag_chain(llm=llm, retriever=retriever, prompt=prompt)\n",
    "    \n",
    "    result = rag_chain.stream({\"question\": question, \"chat_history\":chat_history})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def is_github_url_valid(url):\n",
    "    try:\n",
    "        response = requests.head(url)\n",
    "        if response.status_code == 200:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except requests.exceptions.RequestException:\n",
    "        return False\n",
    "\n",
    "# Example usage\n",
    "github_url = \"https://github.com/alferov/awesome-gulp\"\n",
    "if is_github_url_valid(github_url):\n",
    "    print(f\"{github_url} is a valid GitHub URL.\")\n",
    "else:\n",
    "    print(f\"{github_url} is not a valid GitHub URL.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
