{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.memory import ConversationSummaryMemory\n",
    "# from langchain.chains import ConversationalRetrievalChain\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import AIMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.environ.get('OPENAI_API_KEY')\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.environ.get('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clone an repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<git.repo.base.Repo '/home/sumgh/Data Science/projects/code-analyzer/research/test_repo/.git'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_path = \"test_repo/\"\n",
    "\n",
    "Repo.clone_from(\"https://github.com/Sumegh20/myIPYNBrenderer.git\", to_path=repo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GenericLoader.from_filesystem(path=\"test_repo/src/myIPYNBrenderer\",\n",
    "                                        glob = \"**/*\",\n",
    "                                       suffixes=[\".py\"],\n",
    "                                       parser = LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
    ")\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_splitter = RecursiveCharacterTextSplitter.from_language(language = Language.PYTHON,\n",
    "                                                             chunk_size = 1000,\n",
    "                                                             chunk_overlap = 200)\n",
    "\n",
    "text_splitter = documents_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embdding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=OpenAIEmbeddings(disallowed_special=())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = \"faiss_index\"\n",
    "vectordb = FAISS.from_documents(text_splitter, embeddings)\n",
    "vectordb.save_local(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kn_base = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=kn_base.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"\"\"You are an assistant for question-answering tasks while prioritizing a seamless user experience.\n",
    "            Use the following pieces of retrieved context to answer the question.\n",
    "            If you don't know the answer, just say that you don't know.\n",
    "            Use five sentences maximum and keep the answer concise.\n",
    "            You should be able to remember and reference the last three conversations between you and the user.\n",
    "            Maintain a friendly, positive, and professional tone throughout interactions.\n",
    "            Question: {question}\n",
    "            Context: {context}\n",
    "            Chat history: {chat_history}\n",
    "            Answer:\n",
    "        \"\"\"\n",
    "prompt=ChatPromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0) #, streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_handeler(input: dict):\n",
    "    return input['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "            # {\"context\": retriever,  \"question\": RunnablePassthrough()}\n",
    "            RunnablePassthrough().assign(\n",
    "                context = input_handeler | retriever\n",
    "            )\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your second previous question was about explaining the render_site function.\n"
     ]
    }
   ],
   "source": [
    "question = \"What was my second previous question?\"\n",
    "responce = rag_chain.invoke({\"question\": question, \"chat_history\":chat_history})\n",
    "\n",
    "print(responce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            role=\"user\", \n",
    "            content=question\n",
    "        ),\n",
    "        AIMessage(\n",
    "            role=\"assistant\",\n",
    "            content=responce\n",
    "        ),\n",
    "    ]  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clone any github repositories \n",
    "def repo_ingestion(repo_url):\n",
    "    os.makedirs(\"repo\", exist_ok=True)\n",
    "    repo_path = \"repo/\"\n",
    "    Repo.clone_from(repo_url, to_path=repo_path)\n",
    "\n",
    "\n",
    "#Loading repositories as documents\n",
    "def load_repo(repo_path):\n",
    "    loader = GenericLoader.from_filesystem(repo_path,\n",
    "                                        glob = \"**/*\",\n",
    "                                       suffixes=[\".py\"],\n",
    "                                       parser = LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
    "                                        )\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "\n",
    "#Creating text chunks \n",
    "def text_splitter(documents):\n",
    "    documents_splitter = RecursiveCharacterTextSplitter.from_language(language = Language.PYTHON,\n",
    "                                                             chunk_size = 1000,\n",
    "                                                             chunk_overlap = 200)\n",
    "    \n",
    "    text_chunks = documents_splitter.split_documents(documents)\n",
    "    return text_chunks\n",
    "\n",
    "\n",
    "#loading embeddings model\n",
    "def load_embedding():\n",
    "    embeddings=OpenAIEmbeddings(disallowed_special=())\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "#creating knowlagebase\n",
    "def create_knowledgebase(texts, db_path=\"faiss_index\"):\n",
    "    embeddings = load_embedding()\n",
    "\n",
    "    vectordb = FAISS.from_documents(texts, embeddings)\n",
    "    vectordb.save_local(db_path)\n",
    "\n",
    "\n",
    "#loading knowledgebase\n",
    "def get_knowledge_base(db_path, embedding):\n",
    "    vectordb = FAISS.load_local(db_path, embedding, allow_dangerous_deserialization=True)\n",
    "\n",
    "    return vectordb\n",
    "\n",
    "\n",
    "#Create Prompt\n",
    "\n",
    "#Create RAG chain\n",
    "\n",
    "#user_input"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
